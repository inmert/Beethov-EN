### * Single person project so 100% effort was put in by me.
# Timeline

## August/September: Come up with concept
In this phase, I brainstormed and defined the concept for the musical AI model I want to develop. This involved identifying specific musical tasks, genres, or styles the model will be designed to handle.

## September/October: Create Baseline Model
Once the concept was defined, I started working on the initial version of the model. This phase will involve developing the architecture, algorithms, and training data required to create the baseline model.

## October/November: Improve musical technicalities in the Baseline Model
After the baseline model is created, I focused on improving the technical aspects of the model. This could involve refining the model's ability to recognize and respond to different musical features such as rhythm, melody, harmony, and dynamics.

## November/December: Working on multi-instrument support
In this phase, I added support for multiple instruments. This involved developing new algorithms or modifying existing ones to handle different instrument sounds and playing techniques.

## January/February: Create MelodyNet for Multi-Instrument Support
MelodyNet is a specific module that was developed to support multi-instrument music generation. This phase involved creating and training the MelodyNet module using data from various instruments.

## February/March: Create Variable Audio Encoder for humanizing the outputs
To make the generated music sound more human-like, I developd a Variable Audio Encoder that can introduce variations in the generated audio outputs. This involved adding random variations in pitch, tempo, or other musical features to mimic the subtle nuances of human performance.
